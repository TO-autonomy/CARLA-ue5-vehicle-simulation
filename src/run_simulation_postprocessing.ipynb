{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "import json\n",
    "import shutil\n",
    "import argparse\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import concurrent.futures\n",
    "from queue import Queue\n",
    "from threading import Lock\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from classes.CARLASemantics import SemanticTags, SemanticColors\n",
    "from classes.carla.Sensors import Camera\n",
    "from classes.util.URDFParser import URDFParser\n",
    "from classes.util.Viewshed3D import ViewShed3D, compute_camera_matrix_4x4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Command-line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the arguments if running from command line or \n",
    "# use default values if running from Jupyter Notebook\n",
    "if 'ipykernel_launcher.py' in sys.argv[0]: \n",
    "    args = argparse.Namespace(\n",
    "        ego_vehicle_extrinsics='/home/leppsalu/Desktop/Github/voxel-visibility-multithreaded/CARLA-vehicle-simulation/src/config/carla_extrinsics.urdf',\n",
    "        ego_vehicle_intrinsics='/home/leppsalu/Desktop/Github/voxel-visibility-multithreaded/CARLA-vehicle-simulation/src/config/carla_intrinsics.json',\n",
    "        input_dir='/media/leppsalu/SSD_Storage/generated_data_town03_sample',\n",
    "        output_dir='/media/leppsalu/SSD_Storage/processed_data_town03_sample',\n",
    "        batch_size=15,\n",
    "        n_workers=-1,\n",
    "        n_frames_per_bag=1800,\n",
    "        process_from_frame=0,\n",
    "        mode='minimal',\n",
    "        clean_source_dirs_before_processing=False\n",
    "    )\n",
    "else:\n",
    "    # Create the parser\n",
    "    parser = argparse.ArgumentParser(description='Run simulation postprocessing.')\n",
    "    # Add arguments\n",
    "    parser.add_argument('--ego_vehicle_extrinsics', type=str, required=True,\n",
    "                        help='Path to the ego vehicle extrinsics file')\n",
    "    parser.add_argument('--ego_vehicle_intrinsics', type=str, required=True,\n",
    "                        help='Path to the ego vehicle intrinsics file')\n",
    "    parser.add_argument('--input_dir', type=str, required=True,\n",
    "                        help='Path to the directory with simulation data')\n",
    "    parser.add_argument('--output_dir', type=str, required=True,\n",
    "                        help='Path to the output directory where the processed data will be saved')\n",
    "    parser.add_argument('--batch_size', type=int, required=False, default=15\n",
    "                        , help='Number of frames to process in a single batch (by default 15 frames). For efficient (multi)processing the batch size should not be smaller than number of workers.')\n",
    "    parser.add_argument('--n_workers', type=int, required=False, default=-1,\n",
    "                        help='Number of workers to use for multiprocessing (by default all available cores are used)')\n",
    "    parser.add_argument('--n_frames_per_bag', type=int, required=False, default=1800,\n",
    "                        help='Number of frames in a bag (whole dataset is split into bags of this size; by default 1800 frames are in a bag)')\n",
    "    parser.add_argument('--process_from_frame', type=int, required=False, default=0,\n",
    "                        help='Index of the frame to start processing from (by default 0)')\n",
    "    parser.add_argument(\n",
    "                        '--mode', choices=['minimal', 'full', 'debug'],  default='debug',\n",
    "                        help=\"Select the mode: 'minimal', 'full', or 'debug' (default: 'debug'). Minimal mode only processes depth camera data, full mode processes both depth camera and LiDAR data, debug mode processes both depth camera and LiDAR data and saves intermediate results/files.\"\n",
    "    )\n",
    "    parser.add_argument('--clean_input_dir', action='store_true',\n",
    "                        help='If set, the input directory is cleaned before processing. This removes files that are created in previous post-processing runs if there are any. This does NOT remove the original data files.')\n",
    "    \n",
    "    # Parse the arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "SOURCE_DIR = args.input_dir\n",
    "TARGET_DIR = args.output_dir \n",
    "EXTRINSICS_FILEPATH = args.ego_vehicle_extrinsics\n",
    "INTRINSICS_FILEPATH = args.ego_vehicle_intrinsics\n",
    "N_FRAMES_PER_BAG = args.n_frames_per_bag\n",
    "BATCH_SIZE = args.batch_size\n",
    "N_WORKERS = args.n_workers\n",
    "PROCESS_FROM_FRAME = args.process_from_frame\n",
    "DATA_PROCESING_MODE = args.mode\n",
    "CLEAN_SOURCE_DIR = args.clean_input_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIDAR_DIR = \"LIDAR_TOP\"\n",
    "CAM_DIRS = [\"CAM_FRONT\", \"CAM_FRONT_LEFT\", \"CAM_FRONT_RIGHT\", \"CAM_BACK\", \"CAM_BACK_LEFT\", \"CAM_BACK_RIGHT\"]\n",
    "SEMANTIC_CAM_DIRS =  [\"SEMANTIC_CAM_FRONT\", \"SEMANTIC_CAM_FRONT_LEFT\", \"SEMANTIC_CAM_FRONT_RIGHT\", \"SEMANTIC_CAM_BACK\", \"SEMANTIC_CAM_BACK_LEFT\", \"SEMANTIC_CAM_BACK_RIGHT\"]\n",
    "DEPTH_CAM_DIRS = [\"DEPTH_CAM_FRONT\", \"DEPTH_CAM_FRONT_LEFT\", \"DEPTH_CAM_FRONT_RIGHT\", \"DEPTH_CAM_BACK\", \"DEPTH_CAM_BACK_LEFT\", \"DEPTH_CAM_BACK_RIGHT\"]\n",
    "DEPTH_BEV_DIR = \"DEPTH_BEV\"\n",
    "DEPTH_VISIBILITY_DIR = \"DEPTH_VISIBILITY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve sensor configurations (INTRINSICS and EXTRINSICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRINSICS = URDFParser(EXTRINSICS_FILEPATH)\n",
    "INTRINSICS = dict()\n",
    "with open(INTRINSICS_FILEPATH, \"r\") as INTRINSICS_FILE:\n",
    "    INTRINSICS = json.load(INTRINSICS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intrinsics_dict(query_name):\n",
    "    sensor_name = query_name\n",
    "    if \"DEPTH_\" in sensor_name:\n",
    "        sensor_name = sensor_name.replace(\"DEPTH_\", \"\") # Depth camera and camera intrinsics are the same\n",
    "    if \"SEMANTIC_\" in sensor_name:\n",
    "        sensor_name = sensor_name.replace(\"SEMANTIC_\", \"\") # Semantic camera and camera intrinsics are the same\n",
    "    return INTRINSICS[sensor_name]\n",
    "\n",
    "def get_intrinsics_matrix(query_name):\n",
    "    sensor_name = query_name\n",
    "    if \"DEPTH_\" in sensor_name:\n",
    "        sensor_name = sensor_name.replace(\"DEPTH_\", \"\") # Depth camera and camera intrinsics are the same\n",
    "    if \"SEMANTIC_\" in sensor_name:\n",
    "        sensor_name = sensor_name.replace(\"SEMANTIC_\", \"\") # Semantic camera and camera intrinsics are the same\n",
    "    \n",
    "    camera_intrinsics = INTRINSICS[sensor_name]\n",
    "    fx = camera_intrinsics.get('fx', camera_intrinsics.get('fl'))\n",
    "    fy = camera_intrinsics.get('fy', camera_intrinsics.get('fl'))\n",
    "    w, h = camera_intrinsics.get('w'), camera_intrinsics.get('h')\n",
    "    ppx = w / 2\n",
    "    ppy = h / 2\n",
    "    d_type = camera_intrinsics['disto_type']\n",
    "    D = np.array(camera_intrinsics['disto'])\n",
    "\n",
    "    intrinsics_matrix = np.array([[fx, 0, ppx, 0],\n",
    "                                [0, fy, ppy, 0],\n",
    "                                [0, 0, 1, 0]], dtype=int)\n",
    "    return intrinsics_matrix\n",
    "\n",
    "def get_extrinsics_matrix(query_name):\n",
    "    sensor_name = query_name\n",
    "    if \"DEPTH_\" in sensor_name:\n",
    "        sensor_name = sensor_name.replace(\"DEPTH_\", \"\") # Depth camera and camera intrinsics are the same\n",
    "    if \"SEMANTIC_\" in sensor_name:\n",
    "        sensor_name = sensor_name.replace(\"SEMANTIC_\", \"\") # Semantic camera and camera intrinsics are the same\n",
    "    robot = EXTRINSICS.robot\n",
    "    root_link = EXTRINSICS.root\n",
    "    extrinsics_matrix = EXTRINSICS.compute_chain_transform(robot.get_chain(root_link, sensor_name))\n",
    "    return extrinsics_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post processing constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE = 104         # Output grid size = 104x104 pixels\n",
    "GRID_RESOLUTION = 0.5   # Output grid resolution = 0.5x0.5 meters\n",
    "GRID_ORIGIN = np.array([GRID_SIZE // 2, GRID_SIZE // 2]) \n",
    "GRID_DIAGONAL = np.sqrt(GRID_SIZE**2 + GRID_SIZE**2)\n",
    "MAX_POSTPROCESSING_DISTANCE = GRID_RESOLUTION * np.ceil(GRID_DIAGONAL / 2) # Process only points within this distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filesystem methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files_in_dir(data_dir, string_to_find):\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if string_to_find in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    # print(f\"Removed: {file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error removing {file_path}: {e}\")\n",
    "\n",
    "def get_all_filenames(dir, no_extension=False):\n",
    "    if no_extension:\n",
    "        return [filename.split(\".\")[0] for filename in os.listdir(dir)]\n",
    "    return [filename for filename in os.listdir(dir)]\n",
    "\n",
    "def clean_up_lidar_dir():\n",
    "    LIDAR_DIR_PATH = os.path.join(SOURCE_DIR, LIDAR_DIR)\n",
    "    remove_files_in_dir(LIDAR_DIR_PATH, \".bev.\")\n",
    "    remove_files_in_dir(LIDAR_DIR_PATH, \".ground.\")\n",
    "\n",
    "def clean_up_camera_dirs():\n",
    "    for CAM_DIR in CAM_DIRS:\n",
    "        CAM_DIR_PATH = os.path.join(SOURCE_DIR, CAM_DIR)\n",
    "        remove_files_in_dir(CAM_DIR_PATH, \".pointcloud.\")\n",
    "        remove_files_in_dir(CAM_DIR_PATH, \".visibility.\")\n",
    "        remove_files_in_dir(CAM_DIR_PATH, \".fov.\")\n",
    "\n",
    "def clean_up_depth_camera_dirs():\n",
    "    for DEPTH_CAM_DIR in DEPTH_CAM_DIRS:\n",
    "        DEPTH_CAM_DIR_PATH = os.path.join(SOURCE_DIR, DEPTH_CAM_DIR)\n",
    "        remove_files_in_dir(DEPTH_CAM_DIR_PATH, \".ply\")\n",
    "        remove_files_in_dir(DEPTH_CAM_DIR_PATH, \".fov.\")\n",
    "        remove_files_in_dir(DEPTH_CAM_DIR_PATH, \".visibility.\")\n",
    "\n",
    "def clean_up_depth_bev_dir():\n",
    "    DEPTH_BEV_DIR_PATH = os.path.join(SOURCE_DIR, DEPTH_BEV_DIR)\n",
    "    remove_files_in_dir(DEPTH_BEV_DIR_PATH, \".\")\n",
    "\n",
    "def clean_up_depth_visibility_dir():\n",
    "    DEPTH_VISIBILITY_DIR_PATH = os.path.join(SOURCE_DIR, DEPTH_VISIBILITY_DIR)\n",
    "    remove_files_in_dir(DEPTH_VISIBILITY_DIR_PATH, \".\")\n",
    "        \n",
    "def save_point_cloud(file_path, point_cloud):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    if type(point_cloud) is o3d.geometry.PointCloud:\n",
    "        o3d.io.write_point_cloud(file_path, point_cloud)\n",
    "        return\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "    o3d.io.write_point_cloud(file_path, pcd)\n",
    "\n",
    "def read_point_cloud(file_path):\n",
    "    point_cloud = o3d.io.read_point_cloud(file_path)\n",
    "    return np.asarray(point_cloud.points)\n",
    "\n",
    "def save_image(file_path, mask):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    cv2.imwrite(file_path, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth from depth camera point clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depth image parsing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file_path, color_format=\"BGR\"):\n",
    "    image = None\n",
    "    if color_format == \"BGR\":\n",
    "        image = cv2.imread(file_path, cv2.IMREAD_COLOR)\n",
    "    elif color_format == \"RGB\":\n",
    "        image = plt.imread(file_path)\n",
    "    elif color_format == \"RGBA\":\n",
    "        image = plt.imread(file_path, cv2.IMREAD_UNCHANGED)\n",
    "    elif color_format == \"GRAY\":\n",
    "        image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    assert image is not None, f\"Error reading image from {file_path}\"\n",
    "    return image\n",
    "\n",
    "def calculate_depth_map_from_image(image):\n",
    "    array = image.astype(np.float32)\n",
    "    # Apply (R + G * 256 + B * 256 * 256) / (256 * 256 * 256 - 1).\n",
    "    normalized_depth = np.dot(array[:, :, :3], [65536.0, 256.0, 1.0])\n",
    "    normalized_depth /= 16777215.0  # (256.0 * 256.0 * 256.0 - 1.0)\n",
    "    meters_depth = 1000 * normalized_depth\n",
    "    return meters_depth\n",
    "\n",
    "def clip_depth_map(depth_map, clip_distance):\n",
    "    depth_map[depth_map > clip_distance] = clip_distance\n",
    "    return depth_map\n",
    "\n",
    "def threshold_depth_map(depth_map, max_distance):\n",
    "    depth_map[depth_map > max_distance] = 0.0\n",
    "    return depth_map\n",
    "\n",
    "def create_o3d_pinhole_camera_intrinsics(camera_intrinsics):\n",
    "    height, width = camera_intrinsics[\"h\"], camera_intrinsics[\"w\"]\n",
    "    focal_length = camera_intrinsics[\"fl\"]\n",
    "    def calculate_fov(focal_length, image_width):\n",
    "        fov_radians = 2 * np.arctan(image_width / (2 * focal_length))\n",
    "        fov_degrees = np.degrees(fov_radians)\n",
    "        return fov_degrees\n",
    "    fov = calculate_fov(focal_length, width)\n",
    "    fx = fy = 0.5 * width / np.tan(0.5 * np.radians(fov))\n",
    "    cx = width / 2.0\n",
    "    cy = height / 2.0\n",
    "    o3d_intrinsic = o3d.camera.PinholeCameraIntrinsic(width, height, fx, fy, cx, cy)\n",
    "    return o3d_intrinsic\n",
    "\n",
    "def depth_map_to_point_cloud(depth_map, camera_intrinsics):\n",
    "    height, width = camera_intrinsics[\"h\"], camera_intrinsics[\"w\"]\n",
    "    focal_length = camera_intrinsics[\"fl\"]\n",
    "    def calculate_fov(focal_length, image_width):\n",
    "        fov_radians = 2 * np.arctan(image_width / (2 * focal_length))\n",
    "        fov_degrees = np.degrees(fov_radians)\n",
    "        return fov_degrees\n",
    "    fov = calculate_fov(focal_length, width)\n",
    "    fx = fy = 0.5 * width / np.tan(0.5 * np.radians(fov))\n",
    "    cx = width / 2.0\n",
    "    cy = height / 2.0\n",
    "    u, v = np.meshgrid(np.arange(depth_map.shape[1]), np.arange(depth_map.shape[0]))\n",
    "    \n",
    "    depth_map_points = np.zeros((depth_map.shape[0], depth_map.shape[1], 3), dtype=np.float32)\n",
    "    Z = depth_map\n",
    "    X = (u - cx) * Z / fx\n",
    "    Y = (v - cy) * Z / fy\n",
    "    depth_map_points[:, :, 0] = X\n",
    "    depth_map_points[:, :, 1] = Y\n",
    "    depth_map_points[:, :, 2] = Z\n",
    "    depth_map_points = depth_map_points.reshape(-1, 3)\n",
    "\n",
    "    point_cloud = o3d.geometry.PointCloud()\n",
    "    point_cloud.points = o3d.utility.Vector3dVector(depth_map_points)\n",
    "    return point_cloud\n",
    "\n",
    "def depth_image_to_point_cloud(depth_image, depth_camera_intrinsics, max_distance=None, clip_distance=None):\n",
    "    depth_map = calculate_depth_map_from_image(depth_image)\n",
    "    if clip_distance is not None:\n",
    "        depth_map = clip_depth_map(depth_map, clip_distance)\n",
    "    if max_distance is not None:\n",
    "        depth_map = threshold_depth_map(depth_map, max_distance)\n",
    "    point_cloud = depth_map_to_point_cloud(depth_map, depth_camera_intrinsics)\n",
    "    return point_cloud\n",
    "\n",
    "def depth_image_to_semantic_point_cloud(depth_image, semantic_image, depth_camera_intrinsics, max_distance=None, clip_distance=None):\n",
    "    point_cloud = depth_image_to_point_cloud(depth_image, depth_camera_intrinsics, max_distance, clip_distance)\n",
    "    point_cloud_semantic_colors = semantic_image.reshape(-1, 3) / 255.0\n",
    "    point_cloud.colors = o3d.utility.Vector3dVector(point_cloud_semantic_colors)\n",
    "    return point_cloud\n",
    "\n",
    "def get_points_from_semantic_point_cloud(point_cloud, semantic_tags=None):\n",
    "    point_cloud_points = np.array(point_cloud.points)\n",
    "    point_cloud_colors = (np.array(point_cloud.colors) * 255).astype(np.uint8) \n",
    "    point_cloud_semantic_tags = point_cloud_colors[:, 2]\n",
    "    if semantic_tags is not None:\n",
    "        semantic_tags = np.array(semantic_tags)\n",
    "        point_cloud_points = point_cloud_points[np.where(np.isin(point_cloud_semantic_tags, semantic_tags))]\n",
    "    return point_cloud_points\n",
    "\n",
    "def get_ground_from_semantic_point_cloud(point_cloud):\n",
    "    ground_semantic_tags = np.array([ \n",
    "            SemanticTags.ROADLINE.value, SemanticTags.ROAD.value, \n",
    "            SemanticTags.SIDEWALK.value, SemanticTags.GROUND.value, \n",
    "            SemanticTags.WATER.value, SemanticTags.TERRAIN.value\n",
    "        ], dtype=np.uint8)\n",
    "    ground_points = get_points_from_semantic_point_cloud(point_cloud, ground_semantic_tags)\n",
    "    ground_point_cloud = o3d.geometry.PointCloud()\n",
    "    ground_point_cloud.points = o3d.utility.Vector3dVector(ground_points)\n",
    "    return ground_point_cloud\n",
    "\n",
    "def get_obstacles_from_semantic_point_cloud(point_cloud):\n",
    "    all_semantic_tags = [semantic_tag.value for semantic_tag in SemanticTags]\n",
    "    ground_and_sky_semantic_tags = np.array([ \n",
    "            SemanticTags.ROADLINE.value, SemanticTags.ROAD.value, \n",
    "            SemanticTags.SIDEWALK.value, SemanticTags.GROUND.value, \n",
    "            SemanticTags.WATER.value, SemanticTags.TERRAIN.value\n",
    "        ], dtype=np.uint8)\n",
    "    obstacles_tags = np.setdiff1d(all_semantic_tags, ground_and_sky_semantic_tags)\n",
    "    obstacle_points = get_points_from_semantic_point_cloud(point_cloud, obstacles_tags)\n",
    "    obstacle_point_cloud = o3d.geometry.PointCloud()\n",
    "    obstacle_point_cloud.points = o3d.utility.Vector3dVector(obstacle_points)\n",
    "    return obstacle_point_cloud\n",
    "    \n",
    "\n",
    "def create_color_mask(image, colors, inverted=False):\n",
    "    mask = np.full((image.shape[0], image.shape[1]), 0, dtype=np.uint8)\n",
    "    \n",
    "    B, G, R = image[:, :, 0], image[:, :, 1], image[:, :, 2]\n",
    "    # Iterate through the list of colors\n",
    "    for color in colors:\n",
    "        # Extract color channels\n",
    "        r, g, b = color\n",
    "        # Create boolean masks for each channel comparison\n",
    "        r_mask = R == r\n",
    "        g_mask = G == g\n",
    "        b_mask = B == b\n",
    "        # Combine channel masks to get the final color mask\n",
    "        color_mask = r_mask & g_mask & b_mask\n",
    "        # Update the overall mask where any color matches\n",
    "        mask[color_mask] = 255\n",
    "\n",
    "    if inverted:\n",
    "        mask = np.where(mask == 0, 255, 0).astype(np.uint8)\n",
    "        return mask\n",
    "    return mask\n",
    "\n",
    "def mask_image(image, image_mask):\n",
    "    masked_image = np.array(image)\n",
    "    masked_image[~image_mask] = [0,0,0] \n",
    "    return masked_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods for creating FOV masks, visibility masks and BEV maps from depth images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(grid, camera_matrix, T, image_size):\n",
    "    homogeneous_grid = np.vstack([grid[i].flatten() for i in range(3)] + [np.ones(grid[0].size)])\n",
    "    tfd_points = np.dot(T, homogeneous_grid)\n",
    "    tfd_points = np.dot(camera_matrix, tfd_points)\n",
    "    mask_z = tfd_points[2] > 0 # Exclude points behind the camera\n",
    "    tfd_points[2][tfd_points[2] == 0] = np.nan  # Replace zeros with NaN to avoid division by zero\n",
    "    projected_points = tfd_points / tfd_points[2]\n",
    "    mask = ((0 <= projected_points[0]) & (projected_points[0] < image_size[0]) &\n",
    "            (0 <= projected_points[1]) & (projected_points[1] < image_size[1]) &\n",
    "            mask_z)\n",
    "    return mask.reshape(grid[0].shape)\n",
    "\n",
    "def get_camera_fov_masks(camera_calibs, lidar_to_cam_tf_list=[], grid_size_m=50, resolution=0.5):\n",
    "    whole_mask = np.zeros((int(grid_size_m), int(grid_size_m)))\n",
    "    x, y = np.meshgrid(np.arange(whole_mask.shape[1]), np.arange(whole_mask.shape[0]))\n",
    "    x = x - whole_mask.shape[1] / 2\n",
    "    y = y - whole_mask.shape[0] / 2\n",
    "    z = np.zeros_like(x)\n",
    "    bev_grid = np.array([x, y, z])\n",
    "    bev_grid = np.expand_dims(bev_grid, axis=-1)\n",
    "\n",
    "    cam_masks = {}\n",
    "\n",
    "    for i, (camera, calib) in enumerate(camera_calibs.items()):\n",
    "        intrinsic = calib['K']\n",
    "\n",
    "        w, h = calib['w'], calib['h']\n",
    "\n",
    "        if lidar_to_cam_tf_list:\n",
    "            cam_T_lidar = np.linalg.inv(lidar_to_cam_tf_list[i])\n",
    "        else:\n",
    "            cam_T_lidar = np.linalg.inv(calib['T'])\n",
    "\n",
    "        mask = generate_mask(bev_grid, camera_matrix=intrinsic, T=cam_T_lidar, image_size=(w, h))\n",
    "        visible_bev = np.array([dim[mask] for dim in bev_grid])\n",
    "\n",
    "        mask_ref = visible_bev[:2]\n",
    "        cam_mask = np.zeros_like(mask)\n",
    "\n",
    "        mask_ref[1] += mask.shape[1] / 2\n",
    "        mask_ref[0] += mask.shape[0] / 2\n",
    "        mask_ref = np.round(mask_ref).astype(int)\n",
    "\n",
    "        cam_mask[mask_ref[1], mask_ref[0]] = 1\n",
    "        cam_mask = cam_mask.squeeze(-1)\n",
    "\n",
    "        cam_mask = np.flip(cam_mask, axis=0) # Mirror the mask y axis (otherwise orientation is mirrored)\n",
    "\n",
    "        cam_masks[camera] = cam_mask\n",
    "    return cam_masks\n",
    "\n",
    "def get_fov_mask(image, transformation_matrix, camera_intrinsics_matrix):\n",
    "    # Example camera calibration data for two cameras\n",
    "    camera_calibs = {\n",
    "        'camera': {\n",
    "            'K': camera_intrinsics_matrix,\n",
    "            'w': image.shape[1],  # Image width\n",
    "            'h': image.shape[0],  # Image height\n",
    "            'T': transformation_matrix #@ rotate_around_z()  # Transformation matrix\n",
    "        }\n",
    "    }\n",
    "    # Generate the camera FOV masks\n",
    "    cam_masks = get_camera_fov_masks(camera_calibs, grid_size_m=GRID_SIZE, resolution=GRID_RESOLUTION)\n",
    "    fov_mask = np.asarray(cam_masks[\"camera\"], dtype=np.uint8) * 255\n",
    "    \n",
    "    return fov_mask\n",
    "\n",
    "def calculate_fov(focal_length, image_width):\n",
    "    fov_radians = 2 * np.arctan(image_width / (2 * focal_length))\n",
    "    fov_degrees = np.degrees(fov_radians)\n",
    "    return fov_degrees\n",
    "\n",
    "def rasterize_to_bev(points, resolution=0.5, grid_size=25):\n",
    "    bev_map = np.zeros((int(grid_size), int(grid_size)))\n",
    "    # Converting to grid coordinates\n",
    "    grid_coords = np.floor(points[:, :2] / resolution).astype(np.int32) + int(grid_size // 2)\n",
    "    \n",
    "    # Ensure that grid coordinates are within the bounds of the BEV map\n",
    "    valid_points = (grid_coords[:, 0] >= 0) & (grid_coords[:, 0] < bev_map.shape[0]) & \\\n",
    "                   (grid_coords[:, 1] >= 0) & (grid_coords[:, 1] < bev_map.shape[1])\n",
    "    \n",
    "    # Populate the BEV map with occupancy\n",
    "    bev_map[grid_coords[valid_points, 1], grid_coords[valid_points, 0]] = 255\n",
    "    # Mirror the BEV y axis of image (otherwise pointclouds are rasterized in mirror image)\n",
    "    bev_map = np.flip(bev_map, axis=0) \n",
    "    return bev_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create point clouds, FOV masks, visibility masks and occupancy maps from depth images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_filenames(dir, no_extension=False):\n",
    "    if no_extension:\n",
    "        return [filename.split(\".\")[0] for filename in os.listdir(dir)]\n",
    "    return os.listdir(dir)\n",
    "\n",
    "def get_timestamps_from_filenames(filenames, sorted_order=True):\n",
    "    timestamps_set = {\n",
    "        int(filename.split(\".\")[0]) for filename in filenames if filename.split(\".\")[0].isdigit()\n",
    "    }\n",
    "    timestamps = list(timestamps_set)\n",
    "    if sorted_order:\n",
    "        timestamps = sorted(timestamps)\n",
    "    return timestamps\n",
    "\n",
    "def get_corrected_point_clouds(obstacles_point_cloud, ground_point_cloud, height_range=(0.3, 1.5)):\n",
    "    def gen_mesh(pcd): \n",
    "        try:\n",
    "            points = np.asarray(pcd.points)\n",
    "        except:\n",
    "            points = pcd\n",
    "        tri = Delaunay(points[:, :2])  # We only use the X and Y coordinates\n",
    "        mesh = o3d.geometry.TriangleMesh()\n",
    "        mesh.vertices = o3d.utility.Vector3dVector(points)\n",
    "        mesh.triangles = o3d.utility.Vector3iVector(tri.simplices)\n",
    "        return mesh\n",
    "    \n",
    "    def mesh_to_cloud_signed_distances(o3d_mesh: o3d.t.geometry.TriangleMesh, cloud: o3d.t.geometry.PointCloud) -> np.ndarray:\n",
    "        scene = o3d.t.geometry.RaycastingScene()\n",
    "        _ = scene.add_triangles(o3d_mesh)\n",
    "        sdf = scene.compute_signed_distance(cloud.point.positions)\n",
    "        return sdf.numpy()\n",
    "\n",
    "    def filter_points_far_from_mesh(pcd, distances, t1, t2):\n",
    "        indices1 = np.where((distances > t1) & (distances <= t2))[0]\n",
    "        indices2 = np.where(distances < t1)[0]\n",
    "        objects = pcd.select_by_index(indices1)\n",
    "        ground = pcd.select_by_index(indices2)\n",
    "        return objects, ground\n",
    "\n",
    "    def remove_points_far_from_mesh(pcd, mesh, height_range=(0.4, 2)):\n",
    "        mesh_t = o3d.t.geometry.TriangleMesh.from_legacy(mesh)\n",
    "        tpcd = o3d.t.geometry.PointCloud.from_legacy(pcd)\n",
    "        sdf = mesh_to_cloud_signed_distances(mesh_t, tpcd)\n",
    "        sdf = np.abs(sdf)\n",
    "        obstacles, ground = filter_points_far_from_mesh(pcd, sdf, *height_range)\n",
    "        return obstacles, ground\n",
    "\n",
    "    try:\n",
    "        ground_mesh = gen_mesh(ground_point_cloud)\n",
    "        obstacles_point_cloud, removed_points = remove_points_far_from_mesh(obstacles_point_cloud, ground_mesh, height_range)\n",
    "        ground_point_cloud += removed_points\n",
    "    except:\n",
    "        pass\n",
    "    return obstacles_point_cloud, ground_point_cloud\n",
    "\n",
    "def get_visible_voxels_point_cloud(point_cloud, cameras):\n",
    "\n",
    "    def pcd_to_voxel_indices(points: np.ndarray) -> np.ndarray:\n",
    "        indices = np.floor(points / GRID_RESOLUTION)\n",
    "        # indices[:, :2] -= GRID_ORIGIN\n",
    "        unique = np.unique(indices, axis=0)\n",
    "        return unique\n",
    "    \n",
    "    def voxel_indices_to_pcd(voxel_indices: np.ndarray) -> np.ndarray:\n",
    "        return (voxel_indices + 0.5) * GRID_RESOLUTION\n",
    "    \n",
    "    voxel_size = GRID_RESOLUTION\n",
    "    voxel_centroids = pcd_to_voxel_indices(np.asarray(point_cloud.points))\n",
    "    voxel_centroids = voxel_indices_to_pcd(voxel_centroids)\n",
    "\n",
    "    viewshed = ViewShed3D(voxel_centroids, voxel_size)\n",
    "\n",
    "    visible_voxels = []\n",
    "    for camera in cameras:\n",
    "        camera_matrix = camera.get_projection_matrix()\n",
    "        camera_image_width = camera.get_native_image_width()\n",
    "        camera_image_height = camera.get_native_image_height()\n",
    "        voxels = viewshed.compute_visible_voxels(camera_matrix, camera_image_width, camera_image_height, bounds=GRID_SIZE//2)\n",
    "        if len(voxels) > 0:\n",
    "            visible_voxels.append(voxels)\n",
    "\n",
    "    visible_voxels_point_cloud = o3d.geometry.PointCloud()\n",
    "    if len(visible_voxels) > 0:\n",
    "        visible_voxels = np.array(np.concatenate(visible_voxels))\n",
    "        visible_voxels_point_cloud.points = o3d.utility.Vector3dVector(visible_voxels)\n",
    "    return visible_voxels_point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(timestamp):\n",
    "    \n",
    "    lidar_transform = np.load(os.path.join(SOURCE_DIR, LIDAR_DIR, f\"{timestamp}.npy\"))\n",
    "          \n",
    "    depth_cameras = []\n",
    "    depth_point_cloud = o3d.geometry.PointCloud()\n",
    "    for (SEM_DIR, DEPTH_DIR, CAM_DIR) in zip(SEMANTIC_CAM_DIRS, DEPTH_CAM_DIRS, CAM_DIRS):\n",
    "        depth_camera_transform_path = os.path.join(SOURCE_DIR, DEPTH_DIR, f\"{timestamp}.npy\")\n",
    "        depth_camera_transform = np.load(depth_camera_transform_path)\n",
    "        depth_image_path = os.path.join(SOURCE_DIR, DEPTH_DIR, f\"{timestamp}.png\")\n",
    "        depth_image = read_image(depth_image_path) \n",
    "        semantic_image_path = os.path.join(SOURCE_DIR, SEM_DIR, f\"{timestamp}.png\")\n",
    "        semantic_image = read_image(semantic_image_path)\n",
    "    \n",
    "        depth_camera_intrinsics = get_intrinsics_dict(DEPTH_DIR)\n",
    "        depth_camera_intrinsics_matrix = get_intrinsics_matrix(DEPTH_DIR)\n",
    "        depth_camera_extrinsics_matrix = np.dot(np.linalg.inv(lidar_transform), depth_camera_transform)\n",
    "        depth_camera = Camera(depth_camera_intrinsics_matrix, depth_camera_extrinsics_matrix, name=DEPTH_DIR)\n",
    "        depth_cameras.append(depth_camera)         \n",
    "        \n",
    "        point_cloud = depth_image_to_semantic_point_cloud(depth_image, semantic_image, depth_camera_intrinsics, clip_distance=MAX_POSTPROCESSING_DISTANCE)\n",
    "        point_cloud.transform(depth_camera_transform)\n",
    "        depth_point_cloud += point_cloud\n",
    "    depth_point_cloud.transform(np.linalg.inv(lidar_transform))\n",
    "    \n",
    "    ground_point_cloud = get_ground_from_semantic_point_cloud(depth_point_cloud)\n",
    "    ground_point_cloud = ground_point_cloud.voxel_down_sample(GRID_RESOLUTION)\n",
    "    obstacles_point_cloud = get_obstacles_from_semantic_point_cloud(depth_point_cloud)\n",
    "    obstacles_point_cloud, ground_point_cloud = get_corrected_point_clouds(obstacles_point_cloud, ground_point_cloud)\n",
    "    if DATA_PROCESING_MODE == \"debug\":\n",
    "        save_point_cloud(os.path.join(SOURCE_DIR, DEPTH_BEV_DIR, f\"{timestamp}.obstacles.ply\"), obstacles_point_cloud)\n",
    "        save_point_cloud(os.path.join(SOURCE_DIR, DEPTH_VISIBILITY_DIR, f\"{timestamp}.ply\"), depth_point_cloud)\n",
    "\n",
    "    visible_voxel_point_cloud = get_visible_voxels_point_cloud(depth_point_cloud, depth_cameras)\n",
    "    visible_voxel_point_cloud, _ = get_corrected_point_clouds(visible_voxel_point_cloud, ground_point_cloud, height_range=(-0.05, 1.5))\n",
    "    if DATA_PROCESING_MODE == \"debug\":\n",
    "        save_point_cloud(os.path.join(SOURCE_DIR, DEPTH_VISIBILITY_DIR, f\"{timestamp}.visibility.ply\"), visible_voxel_point_cloud)\n",
    "    cumulative_visibility_mask = rasterize_to_bev(np.asarray(visible_voxel_point_cloud.points), resolution=GRID_RESOLUTION, grid_size=GRID_SIZE)\n",
    "    save_image(os.path.join(SOURCE_DIR, DEPTH_VISIBILITY_DIR, f\"{timestamp}.png\"), cumulative_visibility_mask)\n",
    "\n",
    "    cumulative_occupancy_image = rasterize_to_bev(np.asarray(obstacles_point_cloud.points), resolution=GRID_RESOLUTION, grid_size=GRID_SIZE)\n",
    "    cumulative_occupancy_image[cumulative_visibility_mask == 0] = 0 # Remove points that are not visible in voxel space\n",
    "    save_image(os.path.join(SOURCE_DIR, DEPTH_BEV_DIR, f\"{timestamp}.bev.png\"), cumulative_occupancy_image)\n",
    "\n",
    "    def get_signed_distance_field_from_occupancy_image(occupancy_image):\n",
    "        img = np.array(occupancy_image)\n",
    "        inv_arr = (255 - np.array(img))\n",
    "        sdf = distance_transform_edt(inv_arr).astype(np.float32)\n",
    "        sdf += 1\n",
    "        sdf = 1 / sdf \n",
    "        return sdf\n",
    "    \n",
    "    def get_sdf_as_image(sdf_array):\n",
    "        sdf_array = (sdf_array / sdf_array.max() * 255).astype(np.uint8)\n",
    "        sdf_image = cv2.applyColorMap(sdf_array, cv2.COLORMAP_JET)\n",
    "        return sdf_image\n",
    "    \n",
    "    sdf = get_signed_distance_field_from_occupancy_image(cumulative_occupancy_image)\n",
    "    np.save(os.path.join(SOURCE_DIR, DEPTH_BEV_DIR, f\"{timestamp}.sdf.npy\"), sdf)\n",
    "    sdf_image = get_sdf_as_image(sdf)\n",
    "    save_image(os.path.join(SOURCE_DIR, DEPTH_BEV_DIR, f\"{timestamp}.sdf.png\"), sdf_image)\n",
    "    \n",
    "    reference_frame_transform = lidar_transform\n",
    "    np.save(os.path.join(SOURCE_DIR, DEPTH_VISIBILITY_DIR, f\"{timestamp}.npy\"), reference_frame_transform)\n",
    "    np.save(os.path.join(SOURCE_DIR, DEPTH_BEV_DIR, f\"{timestamp}.npy\"), reference_frame_transform)\n",
    "    \n",
    "\n",
    "    for CAM_DIR, DEPTH_DIR in zip(CAM_DIRS, DEPTH_CAM_DIRS):\n",
    "        depth_image_path = os.path.join(SOURCE_DIR, DEPTH_DIR, f\"{timestamp}.png\")\n",
    "        depth_image = read_image(depth_image_path)\n",
    "        \n",
    "        depth_camera_transform_path = os.path.join(SOURCE_DIR, DEPTH_DIR, f\"{timestamp}.npy\")\n",
    "        depth_camera_transform = np.load(depth_camera_transform_path)\n",
    "        \n",
    "        lidar_transform_path = os.path.join(SOURCE_DIR, LIDAR_DIR, f\"{timestamp}.npy\")\n",
    "        lidar_transform = np.load(lidar_transform_path)\n",
    "        lidar_transform_inv = np.linalg.inv(lidar_transform)\n",
    "        combined_transform = np.dot(lidar_transform_inv, depth_camera_transform)\n",
    "        \n",
    "        intrinsics_matrix = get_intrinsics_matrix(DEPTH_DIR)\n",
    "        camera_fov_mask = get_fov_mask(depth_image, combined_transform, intrinsics_matrix)\n",
    "        fov_mask_path = os.path.join(SOURCE_DIR, DEPTH_DIR, f\"{timestamp}.fov.png\")\n",
    "        save_image(fov_mask_path, camera_fov_mask)\n",
    "\n",
    "        camera_visibility_mask = np.array((cumulative_visibility_mask > 0) & (camera_fov_mask > 0), dtype=np.uint8) * 255\n",
    "        visibility_mask_path = os.path.join(SOURCE_DIR, DEPTH_DIR, f\"{timestamp}.visibility.png\")\n",
    "        save_image(visibility_mask_path, camera_visibility_mask)\n",
    "\n",
    "    def get_lidar_obstacle_point_cloud(semantic_point_cloud):\n",
    "        # Extract colors from the point cloud\n",
    "        semantic_colors = np.asarray(semantic_point_cloud.colors)\n",
    "        r_channel = semantic_colors[:, 0] * 255\n",
    "        mask = np.isin(\n",
    "            r_channel, \n",
    "            [ \n",
    "                SemanticTags.ROADLINE.value, SemanticTags.ROAD.value, SemanticTags.SIDEWALK.value,\n",
    "                SemanticTags.GROUND.value, SemanticTags.WATER.value, SemanticTags.TERRAIN.value, \n",
    "                SemanticTags.SKY.value\n",
    "            ],\n",
    "            invert=True\n",
    "        )\n",
    "        # Filter the points and colors based on the mask\n",
    "        filtered_points = np.asarray(semantic_point_cloud.points)[mask]\n",
    "        filtered_colors = semantic_colors[mask]\n",
    "        # Create a new point cloud with the filtered points and colors\n",
    "        filtered_point_cloud = o3d.geometry.PointCloud()\n",
    "        filtered_point_cloud.points = o3d.utility.Vector3dVector(filtered_points)\n",
    "        filtered_point_cloud.colors = o3d.utility.Vector3dVector(filtered_colors)\n",
    "        return filtered_point_cloud\n",
    "    \n",
    "    def get_lidar_ground_point_cloud(semantic_point_cloud):\n",
    "        # Extract colors from the point cloud\n",
    "        semantic_colors = np.asarray(semantic_point_cloud.colors)\n",
    "        r_channel = semantic_colors[:, 0] * 255\n",
    "        mask = np.isin(\n",
    "            r_channel, \n",
    "            [\n",
    "                SemanticTags.ROADLINE.value, SemanticTags.ROAD.value, \n",
    "                SemanticTags.SIDEWALK.value, SemanticTags.GROUND.value, \n",
    "                SemanticTags.WATER.value, SemanticTags.TERRAIN.value,\n",
    "                SemanticTags.UNLABELED.value # Unlabeled points are considered ground because CARLA considers UE4 landscapes as UNLABELED\n",
    "            ]\n",
    "        )\n",
    "        # Filter the points and colors based on the mask\n",
    "        filtered_points = np.asarray(semantic_point_cloud.points)[mask]\n",
    "        filtered_colors = semantic_colors[mask]\n",
    "        # Create a new point cloud with the filtered points and colors\n",
    "        filtered_point_cloud = o3d.geometry.PointCloud()\n",
    "        filtered_point_cloud.points = o3d.utility.Vector3dVector(filtered_points)\n",
    "        filtered_point_cloud.colors = o3d.utility.Vector3dVector(filtered_colors)\n",
    "        return filtered_point_cloud\n",
    "    \n",
    "    if (DATA_PROCESING_MODE == \"full\") or (DATA_PROCESING_MODE == \"debug\"):\n",
    "        lidar_point_cloud_path = os.path.join(SOURCE_DIR, LIDAR_DIR, f\"{timestamp}.ply\")\n",
    "        lidar_point_cloud = o3d.io.read_point_cloud(lidar_point_cloud_path)\n",
    "        lidar_ground_point_cloud = get_lidar_ground_point_cloud(lidar_point_cloud)\n",
    "        lidar_obstacle_point_cloud = get_lidar_obstacle_point_cloud(lidar_point_cloud)\n",
    "        lidar_obstacle_point_cloud, lidar_ground_point_cloud = get_corrected_point_clouds(lidar_obstacle_point_cloud, lidar_ground_point_cloud)\n",
    "        if DATA_PROCESING_MODE == \"debug\":\n",
    "            save_point_cloud(os.path.join(SOURCE_DIR, LIDAR_DIR, f\"{timestamp}.obstacles.ply\"), lidar_obstacle_point_cloud)\n",
    "            save_point_cloud(os.path.join(SOURCE_DIR, LIDAR_DIR, f\"{timestamp}.ground.ply\"), lidar_ground_point_cloud)\n",
    "        lidar_bev_image = rasterize_to_bev(np.asarray(lidar_obstacle_point_cloud.points), resolution=GRID_RESOLUTION, grid_size=GRID_SIZE)\n",
    "        lidar_bev_image_path = os.path.join(SOURCE_DIR, LIDAR_DIR, f\"{timestamp}.bev.png\")\n",
    "        save_image(lidar_bev_image_path, lidar_bev_image)\n",
    "\n",
    "def split_array_to_batches(array, batch_size):\n",
    "    num_of_batches = len(array) // batch_size\n",
    "    if num_of_batches < 1:\n",
    "        return np.array([array])\n",
    "    return np.array_split(array, num_of_batches)\n",
    "\n",
    "if CLEAN_SOURCE_DIR:\n",
    "    clean_up_lidar_dir()\n",
    "    clean_up_depth_camera_dirs()\n",
    "    clean_up_depth_bev_dir()\n",
    "    clean_up_depth_visibility_dir()\n",
    "\n",
    "all_filenames = get_all_filenames(os.path.join(SOURCE_DIR, LIDAR_DIR))\n",
    "timestamps = get_timestamps_from_filenames(all_filenames)\n",
    "timestamps = timestamps[PROCESS_FROM_FRAME:]\n",
    "timestamp_batches = split_array_to_batches(timestamps, BATCH_SIZE)\n",
    "\n",
    "print(f\"Post-processing simulation data ...\")\n",
    "for i, timestamp_batch in enumerate(timestamp_batches):\n",
    "    Parallel(n_jobs=N_WORKERS)(delayed(process_data)(timestamp) for timestamp in timestamp_batch)\n",
    "    print(f\"{datetime.now()} Processed {((i+1) / len(timestamp_batches) * 100):.6f}% of frames in the dataset. ({timestamps.index(timestamp_batch[-1]) + 1} out of {len(timestamps)} frames)\")\n",
    "print(f\"Processed data in {SOURCE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export processed files to target directory (in suitable directory tree format for machine learning pipeline) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_CAM_DIRS = [\"CAM_FRONT\", \"CAM_FRONT_LEFT\", \"CAM_FRONT_RIGHT\", \"CAM_BACK\", \"CAM_BACK_LEFT\", \"CAM_BACK_RIGHT\"]\n",
    "TARGET_LIDAR_DIR = \"LIDAR_TOP\"\n",
    "TARGET_FOV_MASKS_DIR = \"fov_masks\"\n",
    "TARGET_BEVS_DIR = \"bevs\"\n",
    "TARGET_SDFS_DIR = \"sdfs\"\n",
    "TARGET_VISIBILITY_MASKS_DIR = \"visibility_masks\"\n",
    "TARGET_CUMULATIVE_MASKS_DIR = \"cumulative_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_target_dir():\n",
    "    if not os.path.exists(TARGET_DIR):\n",
    "        return\n",
    "    for subdir in os.listdir(TARGET_DIR):\n",
    "        shutil.rmtree(os.path.join(TARGET_DIR, subdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_file_to_target_dir(source_file_path, target_file_path):\n",
    "    target_directory_path = os.path.dirname(target_file_path)\n",
    "    os.makedirs(target_directory_path, exist_ok=True)\n",
    "    shutil.copyfile(source_file_path, target_file_path)\n",
    "\n",
    "clean_up_target_dir()\n",
    "\n",
    "all_timestamps = get_all_filenames(os.path.join(SOURCE_DIR, DEPTH_CAM_DIRS[0]), no_extension=True)\n",
    "timestamps = get_timestamps_from_filenames(all_filenames)\n",
    "\n",
    "print(f\"Exporting post-processed data...\")\n",
    "for next_timestamp_index, timestamp in enumerate(timestamps):\n",
    "    if (next_timestamp_index % N_FRAMES_PER_BAG) == 0:\n",
    "        start_timestamp = timestamps[next_timestamp_index]\n",
    "        end_timestamp = timestamps[-1] if (next_timestamp_index+N_FRAMES_PER_BAG-1) >= len(timestamps) else timestamps[next_timestamp_index+N_FRAMES_PER_BAG-1]\n",
    "        TARGET_DIRNAME = os.path.basename(TARGET_DIR)\n",
    "        TARGET_BAG_DIR = f\"{TARGET_DIRNAME}_frames_{start_timestamp}_{end_timestamp}\"\n",
    "    \n",
    "    for CAM_DIR in CAM_DIRS:\n",
    "        source_file_path = os.path.join(SOURCE_DIR, CAM_DIR, f\"{timestamp}.png\")\n",
    "        target_file_path = os.path.join(TARGET_DIR, TARGET_BAG_DIR, CAM_DIR, f\"{timestamp}.png\")\n",
    "        copy_file_to_target_dir(source_file_path, target_file_path)\n",
    "        source_file_path = os.path.join(SOURCE_DIR, CAM_DIR, f\"{timestamp}.npy\")\n",
    "        target_file_path = os.path.join(TARGET_DIR, TARGET_BAG_DIR, CAM_DIR, f\"{timestamp}.npy\")\n",
    "        copy_file_to_target_dir(source_file_path, target_file_path)\n",
    "\n",
    "    source_file_path = os.path.join(SOURCE_DIR, LIDAR_DIR, f\"{timestamp}.ply\")\n",
    "    target_file_path = os.path.join(TARGET_DIR, TARGET_BAG_DIR, LIDAR_DIR, f\"{timestamp}.ply\")\n",
    "    copy_file_to_target_dir(source_file_path, target_file_path)\n",
    "    source_file_path = os.path.join(SOURCE_DIR, LIDAR_DIR, f\"{timestamp}.npy\")\n",
    "    target_file_path = os.path.join(TARGET_DIR, TARGET_BAG_DIR, LIDAR_DIR, f\"{timestamp}.npy\")\n",
    "    copy_file_to_target_dir(source_file_path, target_file_path)\n",
    "    \n",
    "    for CAM_DIR, DEPTH_CAM_DIR in zip(CAM_DIRS, DEPTH_CAM_DIRS):\n",
    "        source_file_path = os.path.join(SOURCE_DIR, DEPTH_CAM_DIR, f\"{timestamp}.fov.png\")\n",
    "        target_file_path = os.path.join(TARGET_DIR, TARGET_BAG_DIR, TARGET_FOV_MASKS_DIR, CAM_DIR, f\"{timestamp}.png\")\n",
    "        copy_file_to_target_dir(source_file_path, target_file_path)\n",
    "        source_file_path = os.path.join(SOURCE_DIR, DEPTH_CAM_DIR, f\"{timestamp}.npy\")\n",
    "        target_file_path = os.path.join(TARGET_DIR, TARGET_BAG_DIR, TARGET_FOV_MASKS_DIR, CAM_DIR, f\"{timestamp}.npy\")\n",
    "        copy_file_to_target_dir(source_file_path, target_file_path)\n",
    "\n",
    "        source_file_path = os.path.join(SOURCE_DIR, DEPTH_CAM_DIR, f\"{timestamp}.visibility.png\")\n",
    "        target_file_path = os.path.join(TARGET_DIR, TARGET_BAG_DIR, TARGET_VISIBILITY_MASKS_DIR, CAM_DIR, f\"{timestamp}.png\")\n",
    "        copy_file_to_target_dir(source_file_path, target_file_path)\n",
    "        source_file_path = os.path.join(SOURCE_DIR, DEPTH_CAM_DIR, f\"{timestamp}.npy\")\n",
    "        target_file_path = os.path.join(TARGET_DIR, TARGET_BAG_DIR, TARGET_VISIBILITY_MASKS_DIR, CAM_DIR, f\"{timestamp}.npy\")\n",
    "        copy_file_to_target_dir(source_file_path, target_file_path)\n",
    "\n",
    "    source_file_path = os.path.join(SOURCE_DIR, DEPTH_BEV_DIR, f\"{timestamp}.bev.png\")\n",
    "    target_file_path = os.path.join(TARGET_DIR, TARGET_BAG_DIR, TARGET_BEVS_DIR, f\"{timestamp}.png\")\n",
    "    copy_file_to_target_dir(source_file_path, target_file_path)\n",
    "\n",
    "    source_file_path = os.path.join(SOURCE_DIR, DEPTH_BEV_DIR, f\"{timestamp}.sdf.npy\")\n",
    "    target_file_path = os.path.join(TARGET_DIR, TARGET_BAG_DIR, TARGET_SDFS_DIR, f\"{timestamp}.npy\")\n",
    "    copy_file_to_target_dir(source_file_path, target_file_path)\n",
    "    source_file_path = os.path.join(SOURCE_DIR, DEPTH_BEV_DIR, f\"{timestamp}.sdf.png\")\n",
    "    target_file_path = os.path.join(TARGET_DIR, TARGET_BAG_DIR, TARGET_SDFS_DIR, f\"{timestamp}.png\")\n",
    "    copy_file_to_target_dir(source_file_path, target_file_path)\n",
    "\n",
    "    source_file_path = os.path.join(SOURCE_DIR, DEPTH_VISIBILITY_DIR, f\"{timestamp}.png\")\n",
    "    target_file_path = os.path.join(TARGET_DIR, TARGET_BAG_DIR, TARGET_VISIBILITY_MASKS_DIR, TARGET_CUMULATIVE_MASKS_DIR, f\"{timestamp}.png\")\n",
    "    copy_file_to_target_dir(source_file_path, target_file_path)\n",
    "\n",
    "    if next_timestamp_index % 10 == 0:\n",
    "        print(f\"{datetime.now()} Exported {((next_timestamp_index+1) / len(unique_timestamps) * 100):.6f}% of frames in the dataset\")\n",
    "\n",
    "print(f\"Exported {((next_timestamp_index+1) / len(unique_timestamps) * 100):.6f}% of frames in the dataset\")\n",
    "print(f\"Exported post-processed data to {TARGET_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Post-processing completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
